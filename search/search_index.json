{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"about/","title":"Introduction","text":"<p>Over past years, I have been exploring about Text-to-Speech models, which takes plain text as input and generates audio waveform. There are a lot of models for converting plain text into speech. To understand how text is converted into speech, we need to understand how text and speech is represented.</p> <p>There are different approach in which we can handle text and same goes for speech. For text, we can use text as raw input without any processing or can convert raw text into phonemes and then feed into the model. Module to convert text to phoneme can be helpful to embed different language specific pronunciation details. </p> <p>Audio data is represented in waveform format. Audio data can be represented in different forms. For working with TTS models, I have majorly dealt with audio waveform which represents audio in time dimension and Mel-Spectrogram which represents audio in frequency domain.</p>"},{"location":"about/#approach","title":"Approach","text":"<p>For building TTS model, there are different approach. Models can be categorised based on the approach with which it is converting the text into speech. For example, early models first convert the text into Mel-Spectrogram. Then vocoder models are used to convert the Mel-Spectrogram to wavefrom, basically converting frequency domain data into time domain. Converting text into audio speech is sequence to sequence problem statement. With early models, there were different aspects for creating training data. TTS models were dependent on text to audio time-frames alignment data. Mapping of a text token/phoneme to the time frame which audio waveform/mel-spectrogram belongs. </p> <p>As we move along the time, in hope of building TTS models which could generate speech of high fidelity, which would capture complex nuances of mapping text tokens to audio frames End-to-End models were developed. These End-to-End models would just take plain text/phonemes as input and will generate audio wavefrom directly. It's architecture are designed as such, it will learn the alignment between text and audio frames while training process. Enough talking is done, let's deep dive into the model, which I am planning to create from scratch. It is different from above mention approach. It is based on diffusion model architecture along with text information coming from BERT model embedding.</p>"},{"location":"about/#e3tts","title":"E3TTS","text":"<p>E3TTS is Easy End-to-End Diffusion based text to speech, a simple and efficient model. It takes plain text and generates an audio waveform through an iterative process of refinement. Unlike many TTS models, this does not rely on any intermediate representations like spectrogram features or alignment information. Instead E3TTS models the temporal structure of the waveform through the diffusion process. Without relying on additional conditioning information, E3 TTS could support flexible latent structure within the given audio. This enables E3 TTS to be easily adapted for zero-shot tasks such as editing without any additional training.</p> <p> UNet Structure: DBlock for downsampling block, UBlock for upsampling block</p> <p>To understand the building blocks of E3-TTS, we need to understand few concepts like Adaptive kernel for CNN layer, FiLM and Efficient UNet.</p>"},{"location":"about/#adaptive-kernel","title":"Adaptive kernel","text":"<p>Standard convolution layer works with static kernel which is initialized at start of the training. As training progresses the weights of kernel are updated via backpropagation. Convolution layers came out to be very effective to capture the features of spatial data (images) or sequential data of large sequence (audio data). The behavior of static kernel tends to be very rigid compared to dynamic kernels which are generated on the fly while training. Adaptive kernels have advantage over static kernels like:</p> <ol> <li> <p>Context Sensitivity: The model can change its processing strategy based on the speaker identity. For example, the filter used to generate a high-pitched child's voice should be different from one used for a deep baritone voice.</p> </li> <li> <p>Parameter Efficiency: Instead of learning 100 different static filters to handle 100 different scenarios, you learn one small \"generator\" network that creates the right filter for the right moment.</p> </li> <li> <p>Temporal Precision: In diffusion models (like E3TTS), the noise level changes at every step. Adaptive kernels allow the network to adjust its \"denoising strength\" precisely according to the time-step t, smoothing out noise aggressively in early steps and preserving fine details in later steps.</p> </li> </ol>"},{"location":"about/#implementation","title":"Implementation","text":"<p>Adaptive kernel is used in E3TTS model conditioned with time and speaker embedding. Let's assume the embedding dimensions of time and speaker to be <code>128</code> and <code>256</code> respectively.</p> <ul> <li>Time embedding: t \\in \\mathbb{R}^{128}</li> <li>Speaker embedding: s \\in \\mathbb{R}^{256}</li> <li>Condition embedding: c \\in \\mathbb{R}^{384}</li> </ul> <p>Condition embedding is created by concatenating the time and speaker embedding by last dimension</p> <pre><code>condition = torch.cat([t, s], dim=-1)\n</code></pre> <p>To create adaptive kernels for convolution layer, we need to create <code>N</code> bank of kernel basis and <code>N</code> mixin-weights. For weights, we pass the condition vector <code>c</code> through affine network and softmax activation. This gives the weights which will be used to sum kernel basis. Kernel basis can be initialized with the shape of <code>(n_basis, out_channel, in_channel, kernel_size)</code></p> <pre><code># Affine Network\naffine_network = nn.Sequential(\n    nn.Linear(time_embedding_dims + speaker_embedding_dims, n_basis),\n    nn.Softmax(dim=-1)\n)\n\n# Basis Kernels\nbasis_kernels = nn.Parameter(\n    torch.randn(n_basis, out_channels, in_channels, kernel_size)\n)\n</code></pre> <p>Affine layer calculates the logits from condition vector.  $$ weights = \\text{Softmax}(w^T*c + b);\\quad Where \\;weights\\in   [0, 1]^{n_{\\text{basis}}}  \\\\ \\text{basis_kernels}\\in\\mathbb{R}^{n_{\\text{basis}}\\times\\text{out_channels}\\times\\text{in_channels}\\times\\text{kernel_size}} $$ Based on these weights, basis kernels are summed to calculate the adaptive kernel which will be used to convolution layer.</p> <pre><code>kernel = torch.einsum(\n    'bn,noik-&gt;boik',\n    weights,\n    basis_kernels\n)\n</code></pre> <p>Below is the full implementation of Adaptive Kernel along with Adaptive Convolution layer module in PyTorch:</p> <pre><code>class AdaptiveKernel(nn.Module):\n\n    def __init__(self, n_basis, in_channels, out_channels, kernel_size, time_embedding_dim, speaker_embedding_dim):\n        super(AdaptiveKernel, self).__init__()\n        self.n_basis = n_basis\n        self.time_embedding_dim = time_embedding_dim\n        self.speaker_embedding_dim = speaker_embedding_dim\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n\n        # Defining the Affine Network\n        self.affine_network = nn.Sequential(\n            nn.Linear(self.time_embedding_dim + self.speaker_embedding_dim, self.n_basis),\n            nn.Softmax(dim=-1)\n        )\n\n        # Define the basis filters\n        # N number of kernels of shape (in_channels, out_channels, kernel_size)\n        self.basis_kernels = nn.Parameter(\n            torch.randn(self.n_basis, self.out_channels, self.in_channels, self.kernel_size)\n        )\n\n\n    def forward(self, time_embedding, speaker_embedding):\n        \"\"\"\n        Args:\n            time_embedding: Tensor of shape (B, time_embedding_dim)\n            speaker_embedding: Tensor of shape (B, speaker_embedding_dim)\n        Returns:\n            Tensor of shape (B, out_channels, in_channels, kernel_size)\n        \"\"\"\n        condition = torch([time_embedding, speaker_embedding], dim=-1)\n\n        mix_weights = self.affine_network(condition)\n\n        adaptive_kernel = torch.einsum(\n            'bn,noik-&gt;boik',\n            mix_weights,\n            self.basis_kernels\n        )\n\n        return adaptive_kernel\n\n\nclass AdaptiveConv1D(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, time_embedding_dim, speaker_embedding_dim, n_basis=8):\n        super(AdaptiveConv1D, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.time_embedding_dim = time_embedding_dim\n        self.speaker_embedding_dim = speaker_embedding_dim\n        self.n_basis = n_basis\n        self.adaptive_kernel = AdaptiveKernel(self.n_basis, self.in_channels, self.out_channels, self.kernel_size, self.time_embedding_dim, self.speaker_embedding_dim)\n\n    def forward(self, x, time_embedding, speaker_embedding):\n        \"\"\"\n        Args:\n            x: Tensor of shape (B, in_channels, L)\n            time_embedding: Tensor of shape (B, time_embedding_dim)\n            speaker_embedding: Tensor of shape (B, speaker_embedding_dim)\n        Returns:\n            Tensor of shape (B, out_channels, L)\n        \"\"\"\n        kernel = self.adaptive_kernel(time_embedding, speaker_embedding)\n        return F.conv1d(x, kernel, stride=1, padding=self.kernel_size // 2)\n</code></pre>"},{"location":"unet/","title":"UNet","text":"<p>UNet model architecture is widely used across different models. This model gained traction from paper on image segmentation which is U-Net: Convolution Networks for Biomedical Image Segmentation. This model has downsampling and upsampling part.   UNet Structure: DBlock for downsampling block, UBlock for upsampling block</p>"},{"location":"unet/#implementation","title":"Implementation","text":"<p>This model takes the image tile and outputs segmentation map. To make this model let's assume the input image tile as following: $$ X \\in \\mathbb{R}^{572 \\times 572} $$</p> <p>First model will have the input convolution in which the input image tile is mapped from 1 channel to 64 channels. Then it will pass through series of downsampling blocks. Output of each downsampling blocks will be appended to a list except the last one, which will be used for skip connection. The output of downsampling Mudulelist will be fed into series of updampling blocks which will give feature map with 64 channels. This is finally fed into the output convolution mapping to the final 2 channels.</p> <pre><code>input_conv = nn.Sequential(\n    nn.Conv2d(1, base_channel, 3, padding=0),\n    nn.Conv2d(base_channel, base_channel, 3, padding=0)\n)\n</code></pre> <p>Input Convolution</p> <pre><code>for idx, mult in enumerate(channel_mult):\n    if idx == 0:\n        in_channel = base_channel\n        out_channel = base_channel * mult\n    else:\n        in_channel = out_channel\n        out_channel = base_channel * mult\n\n    downsampling_blocks.append(\n        nn.Sequential(\n            nn.MaxPool2d(2),\n            nn.Conv2d(in_channel, out_channel, 3, padding=0),\n            nn.Conv2d(out_channel, out_channel, 3, padding=0)\n        )\n    )\n</code></pre> <p>Downsampling Layers</p> <pre><code>rev_channel_mult = channel_mult[::-1]\nfor idx, mult in enumerate(rev_channel_mult):\n    if idx == len(rev_channel_mult) - 1:\n        in_channel = base_channel * mult\n        out_channel = base_channel\n    else:\n        in_channel = base_channel * mult\n        out_channel = base_channel * rev_channel_mult[idx+1]\n\n    upsampling_blocks.append(\n        nn.Sequential(\n            nn.ConvTranspose2d(in_channel, out_channel, 2, stride=2, padding=0),\n            nn.Conv2d(in_channel, out_channel, 3, padding=0),\n            nn.Conv2d(out_channel, out_channel, 3, padding=0)\n        )\n    )\n</code></pre> <p>Upsampling Layers</p> <pre><code>out_conv = nn.Conv2d(base_channel, 2, 3, padding=0)\n</code></pre> <p>Output Convolution</p> <pre><code># Forward Pass\nx = torch.randn(1, 1, 572, 572) # (B, C, H, W)\n\n# Encoder path - store skip connections\nskip_connections = []\n\n# Input Convolution\nh = input_conv(x)\nskip_connections.append(h)\n\nfor i, d_block in enumerate(downsampling_blocks):\n    h = d_block(h)\n\n    if i &lt; len(downsampling_blocks) - 1:\n        skip_connections.append(h)\n\n# Reverse the skip connections to match the order of upsampling order\nskip_connections = skip_connections[::-1]\n\n# Decoder path - use skip connections\nfor i, u_block in enumerate(upsampling_blocks):\n    h = u_block[0](h)\n\n    skip = skip_connections[i]\n\n    # Crop skip connection if needed (to match the spatial dimensions)\n    # This handles cases where the dimensions don't match exactly\n    _, _, h_h, h_w = h.shape\n    _, _, s_h, s_w = skip.shape\n\n    if h_h != s_h or h_w != s_w:\n        # Center crop the skip connection\n        diff_h = (s_h - h_h) // 2\n        diff_w = (s_w - h_w) // 2\n        skip = skip[:, :, diff_h:s_h-diff_h, diff_w:s_w-diff_w]\n\n    # Concatenate along channel dimension\n    h = torch.cat([h, skip], dim=1)\n\n    # Apply remaining convolutions\n    h = u_block[1](h)  # First Conv2d (handles doubled channels)\n    h = u_block[2](h)  # Second Conv2d\n\n# Final output convolution\nh = out_conv(h)\nlogger.info(f\"Final output shape: {h.shape}\")\n</code></pre> <p>Sample for Forward Pass</p>"},{"location":"unet/#final-model","title":"Final Model","text":"<pre><code>class UNet(nn.Module):\n    def __init__(self, channel_mult, base_channel):\n        super(UNet, self).__init__()\n        self.channel_mult = channel_mult\n        self.base_channel = base_channel\n\n        self.input_conv = nn.Sequential(\n            nn.Conv2d(1, base_channel, 3, padding=0),\n            nn.Conv2d(base_channel, base_channel, 3, padding=0)\n        )\n\n        self.downsampling_blocks = nn.ModuleList()\n        self.upsampling_blocks = nn.ModuleList()\n\n        for idx, mult in enumerate(channel_mult):\n            if idx == 0:\n                in_channel = base_channel\n                out_channel = base_channel * mult\n            else:\n                in_channel = out_channel\n                out_channel = base_channel * mult\n\n            self.downsampling_blocks.append(\n                nn.Sequential(\n                    nn.MaxPool2d(2),\n                    nn.Conv2d(in_channel, out_channel, 3, padding=0),\n                    nn.Conv2d(out_channel, out_channel, 3, padding=0)\n                )\n            )\n\n        for idx, mult in enumerate(channel_mult[::-1]):\n            if idx == len(channel_mult[::-1]) - 1:\n                in_channel = base_channel * mult\n                out_channel = base_channel\n            else:\n                in_channel = base_channel * mult\n                out_channel = base_channel * channel_mult[::-1][idx+1]\n\n            self.upsampling_blocks.append(\n                nn.Sequential(\n                    nn.ConvTranspose2d(in_channel, out_channel, 2, stride=2, padding=0),\n                    nn.Conv2d(in_channel, out_channel, 3, padding=0),\n                    nn.Conv2d(out_channel, out_channel, 3, padding=0)\n                )\n            )\n\n        self.out_conv = nn.Conv2d(base_channel, 2, 3, padding=0)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Tensor of shape (B, 1, H, W)\n        Returns:\n            Tensor of shape (B, 2, H, W)\n        \"\"\"\n        h = self.input_conv(x)\n        skip_connections = []\n        for i, d_block in enumerate(self.downsampling_blocks):\n            h = d_block(h)\n\n            if i &lt; len(self.downsampling_blocks) - 1:\n                skip_connections.append(h)\n\n        # Reverse the skip connections to match the order of upsampling order\n        skip_connections = skip_connections[::-1]\n        # Decoder path - use skip connections\n        for i, u_block in enumerate(self.upsampling_blocks):\n            h = u_block[0](h)\n\n            skip = skip_connections[i]\n\n            # Crop skip connection if needed (to match the spatial dimensions)\n            # This handles cases where the dimensions don't match exactly\n            _, _, h_h, h_w = h.shape\n            _, _, s_h, s_w = skip.shape\n\n            if h_h != s_h or h_w != s_w:\n                # Center crop the skip connection\n                diff_h = (s_h - h_h) // 2\n                diff_w = (s_w - h_w) // 2\n                skip = skip[:, :, diff_h:s_h-diff_h, diff_w:s_w-diff_w]\n\n            # Concatenate along channel dimension\n            h = torch.cat([h, skip], dim=1)\n\n            # Apply remaining convolutions\n            h = u_block[1](h)  # First Conv2d (handles doubled channels)\n            h = u_block[2](h)  # Second Conv2d\n\n        # Final output convolution\n        h = self.out_conv(h)\n        return h\n</code></pre>"}]}